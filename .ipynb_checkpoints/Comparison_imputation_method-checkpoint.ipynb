{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5329e4-5efb-4c10-918a-12d7e7af49c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<span style=\"font-size:2em\">Comparison of imputation methods, using deep learning structures and classical ones</span> \n",
    "\n",
    "**Occhipinti Mathieu, Maasai Inria**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb7fdf-a5f5-40b9-bdda-beeeb9204fc4",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7401ad07-7876-496a-a638-127472287424",
   "metadata": {},
   "source": [
    "## Purpose of the notebook \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96fe81e-6e18-4379-b899-4f88d0afdbdb",
   "metadata": {},
   "source": [
    "This notebook aims to provide a brief overview of different imputation methods. \n",
    "\n",
    "In the last year, several imputers using deep learning structures have been implemented and claim to have better performances than former state of the art imputation procedure.\n",
    "\n",
    "We will explain the procedure behind each method (without giving too much mathematical details) and then we will compare them.\n",
    "Lastly, we will study the importance of standardization for deep learning based procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb96e2f-c1f9-480f-98d1-de7ab9d33447",
   "metadata": {},
   "source": [
    "## Imputation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b0af7-ca40-4468-8696-570d20daa9b2",
   "metadata": {},
   "source": [
    "Here is a list of the different methods we will study :\n",
    "\n",
    "* `MIDA` (Multiple Imputation using Denoisive autoencoder) proposed by Lovedeep Gondara ande Wang (2018) in the article [MIDA](https://arxiv.org/abs/1705.02737) and using the Python's package [MIDASpy](https://github.com/MIDASverse/MIDASpy)\n",
    "\n",
    "* `MIWAE` (Missing data importance-weighted autoencoder) proposed by Pierre-Alexandre Mattei and Jes Frellsen (2019) in the article [MIWAE](https://arxiv.org/abs/1812.02633) and implemented in [Github MIWAE](https://github.com/pamattei/miwae).\n",
    "\n",
    "* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f84867-bed5-44e5-90aa-c1a827915710",
   "metadata": {},
   "source": [
    "#  Load the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62d8410b-edbb-4a87-a1ec-4d6670fd2b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's all done\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# For MIDA\n",
    "\n",
    "import MIDASpy as md\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import scipy.stats\n",
    "import scipy.io\n",
    "import scipy.sparse\n",
    "from scipy.io import loadmat\n",
    "import torch.distributions as td\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "     \n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"It's all done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5619788-4523-4adf-acd8-7f9a1fda8f65",
   "metadata": {},
   "source": [
    " First we need to create datasets on which we will be able to compare our imputation methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f1023f-f93b-43a9-a8be-2860285a5e63",
   "metadata": {},
   "source": [
    "# Create a complete dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47add005-1186-4081-bfec-d3733ac1f7f1",
   "metadata": {},
   "source": [
    "## Complete dataset in Python\n",
    "\n",
    "In this part, a complete dataset $X \\in \\mathbb{R}^{n\\times d}$ is generated. Different models can be chosen: \n",
    "\n",
    "* **linear relation** between the covariates. More particularly, \n",
    "\n",
    "    * we generate $(x_1,x_2) \\sim \\mathcal{N}(\\mu,\\Sigma)$, with $x_1,x_2 \\in \\mathbb{R}^n$.\n",
    "    * $X = [x_1,\\dots,x_1,x_2,\\dots,x_2] + \\epsilon,$ with $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2 I_{d\\times d})$. The $k$ first columns are $x_1+\\mathrm{noise}$ and the $d-kd$ last columns are $x_2+\\mathrm{noise}$.\n",
    "    \n",
    "* **non-linear relation** between the covariates. \n",
    "\n",
    "    * $\\forall j \\in \\{1,\\dots,d\\}$, the column $j$ of $X$ is such that $X_{.j}=\\sin\\left(f_n\\left(2\\pi\\frac{(j-1)}{d} + u_n\\right)\\right)+\\epsilon_{.j}$,\n",
    "    with $u_n \\sim \\mathcal{U}[0;2\\pi]$ (uniform law) and $f_n$ is fixed and chosen by the user.\n",
    "    \n",
    "The function in Python **create_complete_dataset** allows to create a complete dataset $X$ in such ways. The arguments are detailed as follows\n",
    "\n",
    "* `seed`: seed to reproduct the simulations.\n",
    "\n",
    "* `n`: number of observations in $X$.\n",
    "\n",
    "* `d`: number of covariates in $X$. \n",
    "\n",
    "* `noise`: variance of the noise $\\epsilon$.\n",
    "\n",
    "* `modSimu`: \"linear\" or \"non-linear\".\n",
    "\n",
    "* `k` $\\in \\lbrack 0,1 \\rbrack $: if `modSimu=\"linear\"`, it is the percentage of columns that are equals to x1.\n",
    "\n",
    "* `mu`: if `modSimu=\"linear\"`, the mean vector of $(x_1,x_2)$.\n",
    "\n",
    "* `Sigma`: if `modSimu=\"linear\"`, the covariance matrix of $(x_1,x_2)$.\n",
    "\n",
    "* `fn`: if `modSimu=\"non-linear\"`, the scale-parameter to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86a60762-a214-42d7-96c9-2d191a6bf24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_complete_dataset(seed,n,d,noise,modSimu,k=None,mu=None,Sigma=None,fn=None):\n",
    "    if (modSimu==\"linear\"):\n",
    "        X=np.random.multivariate_normal(size=n,mean=mu,cov=Sigma)\n",
    "        tab=np.empty((n,d))\n",
    "        for i in range (d):\n",
    "            if i<np.around(k*d):\n",
    "        \n",
    "                tab[:,i]=X[:,0]\n",
    "            else :\n",
    "                tab[:,i]=X[:,1]\n",
    "        tab+=np.array(np.random.normal(loc=0,scale=noise,size=(n,d)))\n",
    "\n",
    "    elif (modSimu==\"non-linear\"):\n",
    "        un=np.random.uniform(low=0,high=2*pi,size=n)\n",
    "        tab=np.empty((n,d))\n",
    "        for i in range(d) :\n",
    "            tab[:,i]=sin(fn*((2*pi*i/d+un)))\n",
    "        tab+=np.array(np.random.normal(loc=0,scale=noise,size=(n,d)))\n",
    "    return tab\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eaa6a7-7f29-485d-9b2a-60c84ee3d031",
   "metadata": {},
   "source": [
    "We can then create a dataset, let set the parameters in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d38e2dd3-2ad1-47f2-8474-7a9511fe0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset parameters\n",
    "\n",
    "seed=641\n",
    "n=250\n",
    "d=7\n",
    "modSimu=\"linear\"\n",
    "k=0.4\n",
    "mu=[1,1]\n",
    "Sigma=np.array([[1,1],[1,4]])\n",
    "\n",
    "\n",
    "data=pd.DataFrame(create_complete_dataset(seed=seed,n=n,d=d,modSimu=modSimu,k=k,mu=mu,Sigma=Sigma,noise=0.6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379dbe95-b54b-49f8-afb6-10f47c8c6c14",
   "metadata": {},
   "source": [
    "Here we choose the portion of the dataset for validation and also several percentages of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b55fd9e0-7c62-4037-a108-c441ef87f733",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "valid_size = 0.3 #Proportion of the dataset for validation\n",
    "t = [0.1,0.2,0.3] # Percentage of NA\n",
    "#version = 10 # Number of versions created for each percentage of NA\n",
    "\n",
    "\n",
    "NA = [str(int(k*100)) for k in t] # Percentage of NA as a character string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae022df-4c0b-4c73-a5cb-b7414d4d8105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5481a998-b641-41d0-b027-9f28e7c4131a",
   "metadata": {},
   "source": [
    "## Split the dataset in training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ec12f6a-c47e-423e-aa5c-5d5fc3d36ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dimension of our data\n",
    "\n",
    "rows, cols = data.shape\n",
    "\n",
    "## We create a an array of 0 to rows-1 randomly \n",
    "\n",
    "shuffled_index = np.random.permutation(rows)\n",
    "\n",
    "## We keep the first 70% or so of the indexes for our training set\n",
    "\n",
    "train_index = shuffled_index[:int(rows*(1-valid_size))]\n",
    "\n",
    "## We keep the last 30% or so of the indexes for our validation set\n",
    "\n",
    "valid_index = shuffled_index[int(rows*(1-valid_size)):]\n",
    "\n",
    "## We select our chosen above rows\n",
    "\n",
    "train_data = data.iloc[train_index, :]\n",
    "valid_data = data.iloc[valid_index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca8e790-63c8-4ef2-bfa2-8dc039bc1c37",
   "metadata": {},
   "source": [
    "## Standardization of our variables with the MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e78dde07-72a6-4ba8-abd2-009376c5a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization of the variables\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "## Collect the min and max of each columns of our dataset\n",
    "\n",
    "scaler.fit(train_data)\n",
    "\n",
    "## We create the final data frame and put the original name of the columns back\n",
    "\n",
    "X_train = pd.DataFrame(scaler.transform(train_data))\n",
    "#X_train.columns = column\n",
    "X_Valid = pd.DataFrame(scaler.transform(valid_data))\n",
    "#X_Valid.columns = column\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac932c6-33e4-4c86-b793-6e23578fefb3",
   "metadata": {},
   "source": [
    "# Introduce Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f4b2d-b0e1-433b-817e-ce23e10e8831",
   "metadata": {},
   "source": [
    "To introduce missing values, we use the function **missing_method** whose principles are provided by the MIDA's [article](https://arxiv.org/abs/1705.02737). It allows us to add missing values following a MCAR or MAR mechanism, whether we want the unaivability of the data to do not depend on the data values (MCAR) or depend on the values of the observed variables (MAR). \n",
    "\n",
    "\n",
    "\n",
    "The function **missing_method** has the following arguments:\n",
    "\n",
    "* `raw_data`: our complete dataset.\n",
    "\n",
    "* `mechanism`: type of the process which causes the lack of data: \"mcar\" or \"mar\".\n",
    "\n",
    "* `method`: uniform or random\n",
    "\n",
    "* `t`: percentage of missing values to introduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2c5a599-76b5-42d0-804a-c06650d4698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_method(raw_data, mechanism='mcar', method='uniform',t=0.2) :\n",
    "    \n",
    "    data = raw_data.copy()\n",
    "    rows, cols = data.shape\n",
    "    \n",
    "    # missingness threshold\n",
    "    \n",
    "    if mechanism == 'mcar' :\n",
    "    \n",
    "        if method == 'uniform' :\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where v<=t \n",
    "            # mask is a matrix where mij equals to True if vij<=t and False otherwise\n",
    "            mask = (v<=t)\n",
    "            data[mask] = np.nan\n",
    "\n",
    "        elif method == 'random' :\n",
    "            # only half of the attributes to have missing value\n",
    "            missing_cols = np.random.choice(cols, cols//2)\n",
    "            c = np.zeros(cols, dtype=bool)\n",
    "            c[missing_cols] = True\n",
    "\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where v<=t\n",
    "            mask = (v<=t)*c\n",
    "            data[mask] = np.nan\n",
    "\n",
    "        else :\n",
    "            print(\"Error : There are no such method\")\n",
    "            raise\n",
    "    \n",
    "    elif mechanism == 'mnar' :\n",
    "        \n",
    "        if method == 'uniform' :\n",
    "            # randomly sample two attributes\n",
    "            sample_cols = np.random.choice(cols, 2)\n",
    "\n",
    "            # calculate ther median m1, m2\n",
    "            m1, m2 = np.median(data[:,sample_cols], axis=0)\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where (v<=t) and (x1 <= m1 or x2 >= m2)\n",
    "            m1 = data[:,sample_cols[0]] <= m1\n",
    "            m2 = data[:,sample_cols[1]] >= m2\n",
    "            m = (m1*m2)[:, np.newaxis]\n",
    "\n",
    "            mask = m*(v<=t)\n",
    "            data[mask] = np.nan\n",
    "\n",
    "\n",
    "        elif method == 'random' :\n",
    "            # only half of the attributes to have missing value\n",
    "            missing_cols = np.random.choice(cols, cols//2)\n",
    "            c = np.zeros(cols, dtype=bool)\n",
    "            c[missing_cols] = True\n",
    "\n",
    "            # randomly sample two attributes\n",
    "            sample_cols = np.random.choice(cols, 2)\n",
    "\n",
    "            # calculate ther median m1, m2\n",
    "            m1, m2 = np.median(data[:,sample_cols], axis=0)\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where (v<=t) and (x1 <= m1 or x2 >= m2)\n",
    "            m1 = data[:,sample_cols[0]] <= m1\n",
    "            m2 = data[:,sample_cols[1]] >= m2\n",
    "            m = (m1*m2)[:, np.newaxis]\n",
    "\n",
    "\n",
    "            \n",
    "            mask = m*(v<=t)*c\n",
    "            data[mask] = np.nan\n",
    "\n",
    "        else :\n",
    "            print(\"Error : There is no such method\")\n",
    "            raise\n",
    "    \n",
    "    else :\n",
    "        print(\"Error : There is no such mechanism\")\n",
    "        raise\n",
    "        \n",
    "    return data, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b54d86-9d68-45a7-acea-47c6b6aac031",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss,mask=missing_method(raw_data=X_train,mechanism=\"mcar\",method=\"uniform\",t=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c31761-2aed-49c1-916b-af4f2fdd30c3",
   "metadata": {},
   "source": [
    "# MIDA algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771af532-2b4d-4c65-8439-0463aae4ce9a",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600e3b5-1f93-43ce-98b3-02fd121edf06",
   "metadata": {},
   "source": [
    "Explain briefly the model from the article and introduce the python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab99f2a-ce3c-4040-a9da-bd708e65b6ee",
   "metadata": {},
   "source": [
    "## How to impute with MIDASpy package "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c09d1-d5d3-4503-b9a9-4c04e4c66ee1",
   "metadata": {},
   "source": [
    "All this package is the work of Lall, Ranjit, and Thomas Robinson (2023) and can be found on this [Github Repository](https://github.com/MIDASverse/MIDASpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fd0c20-3231-491a-9124-ff29f2b4dd1d",
   "metadata": {},
   "source": [
    "### Initialization of the MIDA network with the **Midas()** function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce547e4f-7643-4f34-aebe-27c6ee5c7d1d",
   "metadata": {},
   "source": [
    "Here are the main hyperparameters for the initialization and what you need to know about them.\n",
    "\n",
    "* `layer_structure` : a list of integers which defines our network structure. For instance [256,256,256] denotes a three layers network with 256 nodes for each layer. But be careful, while larger networks can learn more complex data structures, they also increase the training time and are more likely to overfit the data.\n",
    "\n",
    "* `learn_rate` : the learning rate called $\\gamma$ controls the size of the adjustments to weights and biases in each training epochs. A higher value of the learning rate might hasten the training procedure but can lead to a loss of accuracy\n",
    "\n",
    "* `input_drop` : $\\in \\lbrack 0,1 \\rbrack$, What is it exactly ?, A higher value will increase training time but prevent overfitting. Empirically, values between $0.7$ and $0.95$ give the better trade-off between speed and accuracy\n",
    "\n",
    "* `train_batch` : To complete\n",
    "\n",
    "* `seed` : An integer to which Python's pseudo-random generator are set to make reproductible results.\n",
    "\n",
    "Other parameters are available for this function but are not at interest for us, check the  [MIDASpy documentation](https://ranjitlall.github.io/assets/pdf/jss4379.pdf) for more informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c888d628-44b8-43c3-b715-3e5568ee80ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "\n",
    "## Layer structure of our network\n",
    "\n",
    "la_str= [256,256,256] \n",
    "\n",
    "## Learning rate\n",
    "\n",
    "lr=0.00001\n",
    "\n",
    "## Input drop \n",
    "\n",
    "in_d=0.8\n",
    "\n",
    "## Size of the training batches\n",
    "\n",
    "tr_b=32\n",
    "\n",
    "## Seed for pseudo-random generator\n",
    "\n",
    "seed=756\n",
    "\n",
    "## Initializing our Mida imputer \n",
    "\n",
    "Midas_imputer=md.Midas(layer_structure=la_str,input_drop=in_d,train_batch=tr_b,seed=seed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9254bba2-d189-4f9a-92e4-61ceeae9ae8a",
   "metadata": {},
   "source": [
    "### Build our imputation model with the **.build_model()** function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddf8e98-a559-4e32-9d6e-27b4121d7e67",
   "metadata": {},
   "source": [
    "Here are the main parameters for this function :\n",
    "\n",
    "* `imputation_target` : a Dataframe which we want to impute values in.\n",
    "\n",
    "* `verbose` : a boolean to specify whether to print a message in the terminal or not (True by default).\n",
    "\n",
    "Other parameters are used to handle categorical variables. It is not at interest for us, but check [MIDASpy documentation](https://ranjitlall.github.io/assets/pdf/jss4379.pdf) for more informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff652a4c-7ff7-4a8d-8481-5734af75ff66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size index: [7]\n",
      "\n",
      "Computation graph constructed\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MIDASpy.midas_base.Midas at 0x2c3a902d850>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Build our model\n",
    "\n",
    "Midas_imputer.build_model(imputation_target=miss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329216d3-f5e2-4441-bef9-96c47a99bbd2",
   "metadata": {},
   "source": [
    "### Train our model with the .**train_model()** function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56434819-b137-4dc3-9e40-be6d85c530f8",
   "metadata": {},
   "source": [
    "Here are the main parameters for this function : \n",
    "\n",
    "* `training_epochs` : an integer which represents the number of complete passes through our network during the traininng (100 by default)\n",
    "\n",
    "* `verbose` : a boolean to specify whether print messages on the terminal including loss values (True by default)\n",
    "\n",
    "* `verbosity_ival ` : an integer that sets the number of training epochs between each message on our terminal (1 by default)\n",
    "\n",
    "* `excessive` : a boolean that specifies if you want to print loss for each mini-batch (False by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60d599d8-886e-4e8e-be0c-252d6628f7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 3.042523193359375\n",
      "Epoch: 30 , loss: 0.9225078463554383\n",
      "Epoch: 60 , loss: 0.9146872401237488\n",
      "Epoch: 90 , loss: 0.8863114833831787\n",
      "Epoch: 120 , loss: 0.8547657489776611\n",
      "Epoch: 150 , loss: 0.8374620318412781\n",
      "Epoch: 180 , loss: 0.8813617825508118\n",
      "Epoch: 210 , loss: 0.8996375322341919\n",
      "Epoch: 240 , loss: 0.879017448425293\n",
      "Epoch: 270 , loss: 0.8576798558235168\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MIDASpy.midas_base.Midas at 0x2c3a902d850>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Parameters \n",
    "\n",
    "## Number of training epochs\n",
    "\n",
    "tr_ep=300\n",
    "\n",
    "## Verbose\n",
    "\n",
    "vb=True\n",
    "\n",
    "## Verbosity eval\n",
    "\n",
    "vb_ival=30\n",
    "\n",
    "## Excessive\n",
    "\n",
    "exc=False\n",
    "\n",
    "## Train our model\n",
    "\n",
    "Midas_imputer.train_model(training_epochs=tr_ep,verbose=vb,verbosity_ival=vb_ival,excessive=exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4508b08-dfe3-456a-ac77-4b8eaf335ea8",
   "metadata": {},
   "source": [
    "### Generate complete datasets with the **.generate_samples()** function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e6856-05cb-4c6e-80b2-51e58697e7c0",
   "metadata": {},
   "source": [
    "Here are the main parameters for this function :\n",
    "\n",
    "* `m` an integer that indicates the number of complete datasets (m=10 by default)\n",
    "\n",
    "* `verbose` a boolean to specify whether you want messages printed in your terminal (True by default)\n",
    "\n",
    "$\\text{\\color{red} Important thing to know}$ :\n",
    "\n",
    "The m complete datasets are stored in **.output_list()** a list from which they can be easily accessed.\n",
    "\n",
    "Nevertheless, when working with large datasets or limited RAM, you may want to use the **.yield_samples()** function. This function takes the same arguments as **.generate_samples()** but doesn't stored the m complete datasets in memory at the same time. Instead it returns a Python generator from which each dataset can be called sequentially. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a3fedab-a9d9-4070-930d-77013b250b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "## Parameters\n",
    "\n",
    "## The number of complete datasets you want\n",
    "\n",
    "M= 5 \n",
    "\n",
    "## Generate our complete datasets\n",
    "\n",
    "Mida_imputations=Midas_imputer.generate_samples(m=M).output_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5e2779-2563-4a9b-bb1f-d3a4f865717a",
   "metadata": {},
   "source": [
    "# MIWAE algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed108c7-0919-4eaa-a89f-adb3840d516c",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e236262e-4bea-496c-aa45-f9f715c02450",
   "metadata": {},
   "source": [
    "## How to impute with MIWAE methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d769cec-fe8b-4457-806b-8e6a67967bd7",
   "metadata": {},
   "source": [
    "In order to keep this notebook clean, we will import a the **MIWAE.py** file which contains the implementation of the MIWAE procedure. All the code included in this file is extracted from the [MIWAE's Github page](https://github.com/pamattei/miwae) and can be found with more comprehensive informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9a80183-0f95-4fc9-b733-f7ee05920205",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m## Number of IS during training\u001b[39;00m\n\u001b[0;32m     13\u001b[0m K\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m\n\u001b[1;32m---> 15\u001b[0m p_z \u001b[38;5;241m=\u001b[39m td\u001b[38;5;241m.\u001b[39mIndependent(td\u001b[38;5;241m.\u001b[39mNormal(loc\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,scale\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones(d)\u001b[38;5;241m.\u001b[39mcuda()),\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\deep-occhipinti-env\\Lib\\site-packages\\torch\\cuda\\__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "## Hyperparameters of the model \n",
    "\n",
    "## Dimension of the latent space\n",
    "\n",
    "d=1\n",
    "\n",
    "## Number of hidden units (same for all MLP's)\n",
    "\n",
    "h=128\n",
    "\n",
    "## Number of IS during training\n",
    "\n",
    "K=20\n",
    "\n",
    "p_z = td.Independent(td.Normal(loc=torch.zeros(d).cuda(),scale=torch.ones(d).cuda()),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "52f030ce-dd6f-446b-9dda-b2bc9992017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the MIWAE file that contains the relevant functions to build and train the MIWAE model\n",
    "\n",
    "from MIWAE import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04268ff-aefb-4b4a-a1a1-40ef371d665d",
   "metadata": {},
   "source": [
    "### Descriptive of the functions with main parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b450e-6dc7-4b3c-baa2-435d69a8bdd4",
   "metadata": {},
   "source": [
    "In the following section, we will make a comprehensive list of the functions contain in the **MIWAE.py** file with parameters for each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d05f77e-10b4-4064-8c99-a58930137d1d",
   "metadata": {},
   "source": [
    "#### The **mse()** function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482b16a5-c6ff-41e1-a228-df85390818ab",
   "metadata": {},
   "source": [
    "The **mse()** function returns the Mean Squared error of your imputed data regarding the true data.\n",
    "\n",
    "**Parameters** :\n",
    "\n",
    "* `xhat` : your imputed dataset\n",
    "\n",
    "* `xtrue` : your complete dataset\n",
    "\n",
    "* `mask` : a binary matrix of the size of our dataset for which the value is zero if the value is missing in your incomplete dataset and one otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c0ac1-8f07-40b1-b827-8f8ebf4153b6",
   "metadata": {},
   "source": [
    "#### The **prior**() function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a630d6bb-d671-472b-b74b-395692cf1790",
   "metadata": {},
   "source": [
    "The **prior()** returns the prior distribution of our latent variables of our DLVM model.\n",
    "\n",
    "**Parameter**:\n",
    "\n",
    "* `d` : dimension of the latent space of our DLVM.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c5042-7498-4793-8eae-eae12c256d8e",
   "metadata": {},
   "source": [
    "#### The **miwae_loss()** function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8fb38a-f92f-4008-ad1b-94d0444c9597",
   "metadata": {},
   "source": [
    "The **miwae_loss()** function computes the value of the loss function used in the MIWAE algorithm. We won't give the parameters for this one as it is only used during the training of the algorithm and won't be directly called in this notebook. \n",
    "However, the full implementation can be found in the **MIWAE.py** file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654f37e-3ae1-498b-b47f-3898d5bc4aec",
   "metadata": {},
   "source": [
    "#### The **build_encoder_decoder()** function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a2fe6-ab00-49a9-86dd-c4111e5125fb",
   "metadata": {},
   "source": [
    "The **build_endoder_decoder()** function is used to create our encoder and decoder for our DLVM, a key part of of the MIWAE procedure.\n",
    "\n",
    "**Parameters** :\n",
    "\n",
    "* `p` : the number of features of our dataset\n",
    "\n",
    "* `h` : the number of hidden units, same for all the MLP's (h=128 by default)\n",
    "\n",
    "* `d` : the dimension of the latent space (d=1 by default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd8e89d-cc88-4746-b7d2-5abd3d65b946",
   "metadata": {},
   "source": [
    "#### The **build_optimizer()** function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ad5c0a-230e-4d86-8d70-b6cfa4b881f8",
   "metadata": {},
   "source": [
    "The **build_optimizer()** function returns the Adam optimizer of our model.\n",
    "\n",
    "**Parameters** :\n",
    "\n",
    "* `encoder` : the encoder of our model\n",
    "\n",
    "* `decoder` : the decoder of our model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d55ab-6cec-4422-af5c-c91338152a2a",
   "metadata": {},
   "source": [
    "#### The **train_MIWAE()** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b25b6-f3fa-44ed-bf4c-0e3f7c87966b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "957f9d6e-5b2a-45c8-9c30-e14bf826b351",
   "metadata": {},
   "source": [
    "# Naive mean imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e471a3-02f4-4bde-aba5-1e75fd61e7dd",
   "metadata": {},
   "source": [
    "## Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2b2817-253f-48cf-9b98-ae84dafc003d",
   "metadata": {},
   "source": [
    "## Mean imputation using the Scikit-learn's Single imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ca2ef-2a7a-4db0-b655-4bb0b26dc5b5",
   "metadata": {},
   "source": [
    "### Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2551a19-3c71-4ef0-a9ba-fd9d6c07c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing our mean imputer\n",
    "\n",
    "mean_imputer=SimpleImputer(strategy=\"mean\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
