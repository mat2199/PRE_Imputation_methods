{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5329e4-5efb-4c10-918a-12d7e7af49c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<span style=\"font-size:2em\">Comparison of imputation methods, using deep learning structures and classical ones</span> \n",
    "\n",
    "**Occhipinti Mathieu, Maasai Inria**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb7fdf-a5f5-40b9-bdda-beeeb9204fc4",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7401ad07-7876-496a-a638-127472287424",
   "metadata": {},
   "source": [
    "## Purpose of the notebook \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96fe81e-6e18-4379-b899-4f88d0afdbdb",
   "metadata": {},
   "source": [
    "This notebook aims to provide a brief overview of different imputation methods. \n",
    "\n",
    "In the last year, several imputers using deep learning structures have been implemented and claim to have better performances than former state of the art imputation procedure.\n",
    "\n",
    "We will explain the procedure behind each method (without giving too much mathematical details) and then we will compare them.\n",
    "Lastly, we will study the importance of standardization for deep learning based procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb96e2f-c1f9-480f-98d1-de7ab9d33447",
   "metadata": {},
   "source": [
    "## Imputation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b0af7-ca40-4468-8696-570d20daa9b2",
   "metadata": {},
   "source": [
    "Here is a list of the different methods we will study :\n",
    "\n",
    "* `MIDA` (Multiple Imputation using Denoisive autoencoder) proposed by Lovedeep Gondara ande Wang (2018) in the article [MIDA](https://arxiv.org/abs/1705.02737) and using the Python's package [MIDASpy](https://github.com/MIDASverse/MIDASpy)\n",
    "\n",
    "* `MIWAE` (Missing data importance-weighted autoencoder) proposed by Pierre-Alexandre Mattei and Jes Frellsen (2019) in the article [MIWAE](https://arxiv.org/abs/1812.02633) and implemented in [Github MIWAE](https://github.com/pamattei/miwae).\n",
    "\n",
    "* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f84867-bed5-44e5-90aa-c1a827915710",
   "metadata": {},
   "source": [
    "#  Load the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62d8410b-edbb-4a87-a1ec-4d6670fd2b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's all done\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# For MIDA\n",
    "\n",
    "import MIDASpy as md\n",
    "\n",
    "\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"It's all done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5619788-4523-4adf-acd8-7f9a1fda8f65",
   "metadata": {},
   "source": [
    " First we need to create datasets on which we will be able to compare our imputation methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f1023f-f93b-43a9-a8be-2860285a5e63",
   "metadata": {},
   "source": [
    "# Create a complete dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47add005-1186-4081-bfec-d3733ac1f7f1",
   "metadata": {},
   "source": [
    "## Complete dataset in Python\n",
    "\n",
    "In this part, a complete dataset $X \\in \\mathbb{R}^{n\\times d}$ is generated. Different models can be chosen: \n",
    "\n",
    "* **linear relation** between the covariates. More particularly, \n",
    "\n",
    "    * we generate $(x_1,x_2) \\sim \\mathcal{N}(\\mu,\\Sigma)$, with $x_1,x_2 \\in \\mathbb{R}^n$.\n",
    "    * $X = [x_1,\\dots,x_1,x_2,\\dots,x_2] + \\epsilon,$ with $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2 I_{d\\times d})$. The $k$ first columns are $x_1+\\mathrm{noise}$ and the $d-kd$ last columns are $x_2+\\mathrm{noise}$.\n",
    "    \n",
    "* **non-linear relation** between the covariates. \n",
    "\n",
    "    * $\\forall j \\in \\{1,\\dots,d\\}$, the column $j$ of $X$ is such that $X_{.j}=\\sin\\left(f_n\\left(2\\pi\\frac{(j-1)}{d} + u_n\\right)\\right)+\\epsilon_{.j}$,\n",
    "    with $u_n \\sim \\mathcal{U}[0;2\\pi]$ (uniform law) and $f_n$ is fixed and chosen by the user.\n",
    "    \n",
    "The function in Python **create_complete_dataset** allows to create a complete dataset $X$ in such ways. The arguments are detailed as follows\n",
    "\n",
    "* `seed`: seed to reproduct the simulations.\n",
    "\n",
    "* `n`: number of observations in $X$.\n",
    "\n",
    "* `d`: number of covariates in $X$. \n",
    "\n",
    "* `noise`: variance of the noise $\\epsilon$.\n",
    "\n",
    "* `modSimu`: \"linear\" or \"non-linear\".\n",
    "\n",
    "* `k` $\\in \\lbrack 0,1 \\rbrack $: if `modSimu=\"linear\"`, it is the percentage of columns that are equals to x1.\n",
    "\n",
    "* `mu`: if `modSimu=\"linear\"`, the mean vector of $(x_1,x_2)$.\n",
    "\n",
    "* `Sigma`: if `modSimu=\"linear\"`, the covariance matrix of $(x_1,x_2)$.\n",
    "\n",
    "* `fn`: if `modSimu=\"non-linear\"`, the scale-parameter to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86a60762-a214-42d7-96c9-2d191a6bf24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_complete_dataset(seed,n,d,noise,modSimu,k=None,mu=None,Sigma=None,fn=None):\n",
    "    if (modSimu==\"linear\"):\n",
    "        X=np.random.multivariate_normal(size=n,mean=mu,cov=Sigma)\n",
    "        tab=np.empty((n,d))\n",
    "        for i in range (d):\n",
    "            if i<np.around(k*d):\n",
    "        \n",
    "                tab[:,i]=X[:,0]\n",
    "            else :\n",
    "                tab[:,i]=X[:,1]\n",
    "        tab+=np.array(np.random.normal(loc=0,scale=noise,size=(n,d)))\n",
    "\n",
    "    elif (modSimu==\"non-linear\"):\n",
    "        un=np.random.uniform(low=0,high=2*pi,size=n)\n",
    "        tab=np.empty((n,d))\n",
    "        for i in range(d) :\n",
    "            tab[:,i]=sin(fn*((2*pi*i/d+un)))\n",
    "        tab+=np.array(np.random.normal(loc=0,scale=noise,size=(n,d)))\n",
    "    return tab\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eaa6a7-7f29-485d-9b2a-60c84ee3d031",
   "metadata": {},
   "source": [
    "We can then create a dataset, let set the parameters in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d38e2dd3-2ad1-47f2-8474-7a9511fe0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset parameters\n",
    "\n",
    "seed=641\n",
    "n=250\n",
    "d=7\n",
    "modSimu=\"linear\"\n",
    "k=0.4\n",
    "mu=[1,1]\n",
    "Sigma=np.array([[1,1],[1,4]])\n",
    "\n",
    "\n",
    "data=pd.DataFrame(create_complete_dataset(seed=seed,n=n,d=d,modSimu=modSimu,k=k,mu=mu,Sigma=Sigma,noise=0.6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379dbe95-b54b-49f8-afb6-10f47c8c6c14",
   "metadata": {},
   "source": [
    "Here we choose the portion of the dataset for validation and also several percentages of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b55fd9e0-7c62-4037-a108-c441ef87f733",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "valid_size = 0.3 #Proportion of the dataset for validation\n",
    "t = [0.1,0.2,0.3] # Percentage of NA\n",
    "#version = 10 # Number of versions created for each percentage of NA\n",
    "\n",
    "\n",
    "NA = [str(int(k*100)) for k in t] # Percentage of NA as a character string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae022df-4c0b-4c73-a5cb-b7414d4d8105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5481a998-b641-41d0-b027-9f28e7c4131a",
   "metadata": {},
   "source": [
    "## Split the dataset in training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2ec12f6a-c47e-423e-aa5c-5d5fc3d36ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dimension of our data\n",
    "\n",
    "rows, cols = data.shape\n",
    "\n",
    "## We create a an array of 0 to rows-1 randomly \n",
    "\n",
    "shuffled_index = np.random.permutation(rows)\n",
    "\n",
    "## We keep the first 70% or so of the indexes for our training set\n",
    "\n",
    "train_index = shuffled_index[:int(rows*(1-valid_size))]\n",
    "\n",
    "## We keep the last 30% or so of the indexes for our validation set\n",
    "\n",
    "valid_index = shuffled_index[int(rows*(1-valid_size)):]\n",
    "\n",
    "## We select our chosen above rows\n",
    "\n",
    "train_data = data.iloc[train_index, :]\n",
    "valid_data = data.iloc[valid_index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca8e790-63c8-4ef2-bfa2-8dc039bc1c37",
   "metadata": {},
   "source": [
    "## Standardization of our variables with the MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e78dde07-72a6-4ba8-abd2-009376c5a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization of the variables\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "## Collect the min and max of each columns of our dataset\n",
    "\n",
    "scaler.fit(train_data)\n",
    "\n",
    "## We create the final data frame and put the original name of the columns back\n",
    "\n",
    "X_train = pd.DataFrame(scaler.transform(train_data))\n",
    "#X_train.columns = column\n",
    "X_Valid = pd.DataFrame(scaler.transform(valid_data))\n",
    "#X_Valid.columns = column\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac932c6-33e4-4c86-b793-6e23578fefb3",
   "metadata": {},
   "source": [
    "# Introduce Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f4b2d-b0e1-433b-817e-ce23e10e8831",
   "metadata": {},
   "source": [
    "To introduce missing values, we use the function **missing_method** whose principles are provided by the MIDA's [article](https://arxiv.org/abs/1705.02737). It allows us to add missing values following a MCAR or MAR mechanism, whether we want the unaivability of the data to do not depend on the data values (MCAR) or depend on the values of the observed variables (MAR). \n",
    "\n",
    "\n",
    "\n",
    "The function **missing_method** has the following arguments:\n",
    "\n",
    "* `raw_data`: our complete dataset.\n",
    "\n",
    "* `mechanism`: type of the process which causes the lack of data: \"mcar\" or \"mar\".\n",
    "\n",
    "* `method`: uniform or random\n",
    "\n",
    "* `t`: percentage of missing values to introduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2c5a599-76b5-42d0-804a-c06650d4698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_method(raw_data, mechanism='mcar', method='uniform',t=0.2) :\n",
    "    \n",
    "    data = raw_data.copy()\n",
    "    rows, cols = data.shape\n",
    "    \n",
    "    # missingness threshold\n",
    "    \n",
    "    if mechanism == 'mcar' :\n",
    "    \n",
    "        if method == 'uniform' :\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where v<=t \n",
    "            # mask is a matrix where mij equals to True if vij<=t and False otherwise\n",
    "            mask = (v<=t)\n",
    "            data[mask] = np.nan\n",
    "\n",
    "        elif method == 'random' :\n",
    "            # only half of the attributes to have missing value\n",
    "            missing_cols = np.random.choice(cols, cols//2)\n",
    "            c = np.zeros(cols, dtype=bool)\n",
    "            c[missing_cols] = True\n",
    "\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where v<=t\n",
    "            mask = (v<=t)*c\n",
    "            data[mask] = np.nan\n",
    "\n",
    "        else :\n",
    "            print(\"Error : There are no such method\")\n",
    "            raise\n",
    "    \n",
    "    elif mechanism == 'mnar' :\n",
    "        \n",
    "        if method == 'uniform' :\n",
    "            # randomly sample two attributes\n",
    "            sample_cols = np.random.choice(cols, 2)\n",
    "\n",
    "            # calculate ther median m1, m2\n",
    "            m1, m2 = np.median(data[:,sample_cols], axis=0)\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where (v<=t) and (x1 <= m1 or x2 >= m2)\n",
    "            m1 = data[:,sample_cols[0]] <= m1\n",
    "            m2 = data[:,sample_cols[1]] >= m2\n",
    "            m = (m1*m2)[:, np.newaxis]\n",
    "\n",
    "            mask = m*(v<=t)\n",
    "            data[mask] = np.nan\n",
    "\n",
    "\n",
    "        elif method == 'random' :\n",
    "            # only half of the attributes to have missing value\n",
    "            missing_cols = np.random.choice(cols, cols//2)\n",
    "            c = np.zeros(cols, dtype=bool)\n",
    "            c[missing_cols] = True\n",
    "\n",
    "            # randomly sample two attributes\n",
    "            sample_cols = np.random.choice(cols, 2)\n",
    "\n",
    "            # calculate ther median m1, m2\n",
    "            m1, m2 = np.median(data[:,sample_cols], axis=0)\n",
    "            # uniform random vector\n",
    "            v = np.random.uniform(size=(rows, cols))\n",
    "\n",
    "            # missing values where (v<=t) and (x1 <= m1 or x2 >= m2)\n",
    "            m1 = data[:,sample_cols[0]] <= m1\n",
    "            m2 = data[:,sample_cols[1]] >= m2\n",
    "            m = (m1*m2)[:, np.newaxis]\n",
    "\n",
    "\n",
    "            \n",
    "            mask = m*(v<=t)*c\n",
    "            data[mask] = np.nan\n",
    "\n",
    "        else :\n",
    "            print(\"Error : There is no such method\")\n",
    "            raise\n",
    "    \n",
    "    else :\n",
    "        print(\"Error : There is no such mechanism\")\n",
    "        raise\n",
    "        \n",
    "    return data, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85b54d86-9d68-45a7-acea-47c6b6aac031",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss,mask=missing_method(raw_data=X_train,mechanism=\"mcar\",method=\"uniform\",t=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c31761-2aed-49c1-916b-af4f2fdd30c3",
   "metadata": {},
   "source": [
    "# MIDA algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771af532-2b4d-4c65-8439-0463aae4ce9a",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600e3b5-1f93-43ce-98b3-02fd121edf06",
   "metadata": {},
   "source": [
    "Explain briefly the model from the article and introduce the python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab99f2a-ce3c-4040-a9da-bd708e65b6ee",
   "metadata": {},
   "source": [
    "## How to impute with MIDASpy package "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c09d1-d5d3-4503-b9a9-4c04e4c66ee1",
   "metadata": {},
   "source": [
    "All this package is the work of Lall, Ranjit, and Thomas Robinson (2023) and can be found on this [Github Repository](https://github.com/MIDASverse/MIDASpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fd0c20-3231-491a-9124-ff29f2b4dd1d",
   "metadata": {},
   "source": [
    "### Initialization of the MIDA network with the **Midas()** function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce547e4f-7643-4f34-aebe-27c6ee5c7d1d",
   "metadata": {},
   "source": [
    "Here are the main hyperparameters for the initialization and what you need to know about them.\n",
    "\n",
    "* `layer_structure` : a list of integers which defines our network structure. For instance [256,256,256] denotes a three layers network with 256 nodes for each layer. But be careful, while larger networks can learn more complex data structures, they also increase the training time and are more likely to overfit the data.\n",
    "\n",
    "* `learn_rate` : the learning rate called $\\gamma$ controls the size of the adjustments to weights and biases in each training epochs. A higher value of the learning rate might hasten the training procedure but can lead to a loss of accuracy\n",
    "\n",
    "* `input_drop` : $\\in \\lbrack 0,1 \\rbrack$, What is it exactly ?, A higher value will increase training time but prevent overfitting. Empirically, values between $0.7$ and $0.95$ give the better trade-off between speed and accuracy\n",
    "\n",
    "* `train_batch` : To complete\n",
    "\n",
    "* `seed` : An integer to which Python's pseudo-random generator are set to make reproductible results.\n",
    "\n",
    "Other parameters are available for this function but are not at interest for us, check the  [MIDASpy documentation](https://ranjitlall.github.io/assets/pdf/jss4379.pdf) for more informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c888d628-44b8-43c3-b715-3e5568ee80ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "\n",
    "## Layer structure of our network\n",
    "\n",
    "la_str= [256,256,256] \n",
    "\n",
    "## Learning rate\n",
    "\n",
    "lr=0.00001\n",
    "\n",
    "#\" Input drop \n",
    "\n",
    "in_d=0.8\n",
    "\n",
    "## Size of the training batches\n",
    "\n",
    "tr_b=32\n",
    "\n",
    "## Seed for pseudo-random generator\n",
    "\n",
    "seed=756\n",
    "\n",
    "## Initializing our Mida imputer \n",
    "\n",
    "Midas_imputer=md.Midas(layer_structure=la_str,input_drop=in_d,train_batch=tr_b,seed=seed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9254bba2-d189-4f9a-92e4-61ceeae9ae8a",
   "metadata": {},
   "source": [
    "### Build our imputation model with the **.build_model()** function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddf8e98-a559-4e32-9d6e-27b4121d7e67",
   "metadata": {},
   "source": [
    "Here are the main parameters for this function :\n",
    "\n",
    "* `imputation_target` : a Dataframe which we want to impute values in.\n",
    "\n",
    "* `verbose` : a boolean to specify whether to print a message in the terminal or not (True by default).\n",
    "\n",
    "Other parameters are used to handle categorical variables. It is not at interest for us, but check [MIDASpy documentation](https://ranjitlall.github.io/assets/pdf/jss4379.pdf) for more informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ff652a4c-7ff7-4a8d-8481-5734af75ff66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size index: [7]\n",
      "\n",
      "Computation graph constructed\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MIDASpy.midas_base.Midas at 0x2770b16d550>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Build our model\n",
    "\n",
    "Midas_imputer.build_model(imputation_target=miss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329216d3-f5e2-4441-bef9-96c47a99bbd2",
   "metadata": {},
   "source": [
    "### Train our model with the .**train_model()** function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56434819-b137-4dc3-9e40-be6d85c530f8",
   "metadata": {},
   "source": [
    "Here are the main parameters for this function : \n",
    "\n",
    "* `training_epochs` : an integer which represents the number of complete passes through our network during the traininng (100 by default)\n",
    "\n",
    "* `verbose` : a boolean to specify whether print messages on the terminal including loss values (True by default)\n",
    "\n",
    "* `verbosity_ival ` : an integer that sets the number of training epochs between each message on our terminal (1 by default)\n",
    "\n",
    "* `excessive` : a boolean that specifies if you want to print loss for each mini-batch (False by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "60d599d8-886e-4e8e-be0c-252d6628f7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialised\n",
      "\n",
      "Epoch: 0 , loss: 3.0120581150054933\n",
      "Epoch: 30 , loss: 0.9344840049743652\n",
      "Epoch: 60 , loss: 0.9581799030303955\n",
      "Epoch: 90 , loss: 0.9077066898345947\n",
      "Epoch: 120 , loss: 0.917396605014801\n",
      "Epoch: 150 , loss: 0.8803330659866333\n",
      "Epoch: 180 , loss: 0.8384041547775268\n",
      "Epoch: 210 , loss: 0.9247808694839478\n",
      "Epoch: 240 , loss: 0.9308383941650391\n",
      "Epoch: 270 , loss: 0.8914738297462463\n",
      "Training complete. Saving file...\n",
      "Model saved in file: tmp/MIDAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MIDASpy.midas_base.Midas at 0x2770b16d550>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Parameters \n",
    "\n",
    "## Number of training epochs\n",
    "\n",
    "tr_ep=300\n",
    "\n",
    "## Verbose\n",
    "\n",
    "vb=True\n",
    "\n",
    "## Verbosity eval\n",
    "\n",
    "vb_ival=30\n",
    "\n",
    "## Excessive\n",
    "\n",
    "exc=False\n",
    "\n",
    "## Train our model\n",
    "\n",
    "Midas_imputer.train_model(training_epochs=tr_ep,verbose=vb,verbosity_ival=vb_ival,excessive=exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4508b08-dfe3-456a-ac77-4b8eaf335ea8",
   "metadata": {},
   "source": [
    "### Generate complete datasets with the **.generate_samples()** function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e6856-05cb-4c6e-80b2-51e58697e7c0",
   "metadata": {},
   "source": [
    "Here are the main parameters for this function :\n",
    "\n",
    "* `m` an integer that indicates the number of complete datasets (m=10 by default)\n",
    "\n",
    "* `verbose` a boolean to specify whether you want messages printed in your terminal (True by default)\n",
    "\n",
    "$\\text{\\color{red} Important things to know}$ :\n",
    "\n",
    "The m complete datasets are stored in **.output_list()** a list from which they can be easily accessed.\n",
    "\n",
    "Nevertheless, when working with large datasets or limited RAM, you may want to use the **.yield_samples()** function. This function takes the same arguments as **.generate_samples()** but doesn't stored the m complete datasets in memory at the same time. Instead it returns a Python generator from which each dataset can be called sequentially. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8a3fedab-a9d9-4070-930d-77013b250b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/MIDAS\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "## Parameters\n",
    "\n",
    "## The number of complete datasets you want\n",
    "\n",
    "M= 5 \n",
    "\n",
    "## Generate our complete datasets\n",
    "\n",
    "Mida_imputations=Midas_imputer.generate_samples(m=M).output_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2783704c-8595-4bba-aade-a6ed3864dba9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
